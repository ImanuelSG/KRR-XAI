{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification Interpretability (Library-based)\n",
    "\n",
    "This notebook demonstrates interpretability techniques using standard libraries:\n",
    "1. LIME\n",
    "2. SHAP\n",
    "3. Integrated Gradients (IG) - Replaces LRP for better stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch pandas matplotlib numpy lime shap -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from transformers import RobertaTokenizer, AutoModelForSequenceClassification\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './best_roberta_model'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with open(f'{model_path}/label_mappings.json', 'r') as f:\n",
    "    label_mappings = json.load(f)\n",
    "\n",
    "label_list = label_mappings['label_list']\n",
    "label2id = label_mappings['label2id']\n",
    "id2label = {int(k): v for k, v in label_mappings['id2label'].items()}\n",
    "\n",
    "def predict_proba(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    elif isinstance(texts, np.ndarray):\n",
    "        texts = texts.tolist()\n",
    "    \n",
    "    texts = [str(t) for t in texts]\n",
    "    \n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "    return probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "    \"This movie is absolutely fantastic I loved every moment of it\",\n",
    "    \"The state of corruption in our society is utterly disgusting\",\n",
    "    \"The product is perfect it exceeded all my expectations\",\n",
    "    \"I'm extremely happy with this purchase Highly recommend\",\n",
    "    \"This is the worst service I have ever encountered Absolutely horrible\",\n",
    "    \"Bad staff never coming back\",\n",
    "    \"Outstanding quality Best investment I ever made\",\n",
    "    \"The acting was pathetic and the plot was nonsense\",\n",
    "    \"I adore this place it is magical and wonderful\",\n",
    "    \"Envy poisons my thoughts coveting others success\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = LimeTextExplainer(class_names=label_list)\n",
    "\n",
    "def explain_with_lime(text, num_features=5):\n",
    "    probs = predict_proba([text])[0]\n",
    "    pred_class = int(np.argmax(probs))\n",
    "\n",
    "    exp = lime_explainer.explain_instance(\n",
    "        text,\n",
    "        predict_proba,\n",
    "        num_features=num_features,\n",
    "        num_samples=1000,\n",
    "        labels=[pred_class]\n",
    "    )\n",
    "\n",
    "    importance = exp.as_list(label=pred_class)\n",
    "    importance = sorted(importance, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "    return importance, pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.Explainer(predict_proba, shap.maskers.Text(tokenizer))\n",
    "\n",
    "def explain_with_shap(text, num_features=5):\n",
    "    shap_values = shap_explainer([text])\n",
    "    pred_class = np.argmax(predict_proba([text])[0])\n",
    "\n",
    "    values = shap_values.values[0, :, pred_class]\n",
    "    tokens = shap_values.data[0]\n",
    "\n",
    "    word_scores = []\n",
    "    for token, score in zip(tokens, values):\n",
    "        clean_word = str(token).replace('Ġ', '').replace('\\u0120', '').replace('▁', '').strip()\n",
    "        if clean_word and clean_word not in ['<s>', '</s>', '<pad>']:\n",
    "            word_scores.append((clean_word, score))\n",
    "\n",
    "    word_scores.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "    return word_scores[:num_features], pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients (IG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_with_ig(text, num_features=5, n_steps=50):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=64)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    embeddings = embedding_layer(input_ids).detach()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "        pred_class = torch.argmax(probs).item()\n",
    "\n",
    "    baseline = torch.zeros_like(embeddings).to(device)\n",
    "    \n",
    "    total_gradients = torch.zeros_like(embeddings).to(device)\n",
    "    \n",
    "    for step in range(n_steps + 1):\n",
    "        alpha = float(step) / n_steps\n",
    "        interpolated = baseline + alpha * (embeddings - baseline)\n",
    "        interpolated = interpolated.requires_grad_(True)\n",
    "        \n",
    "        outputs = model(inputs_embeds=interpolated, attention_mask=attention_mask)\n",
    "        score = outputs.logits[0, pred_class]\n",
    "        \n",
    "        score.backward()\n",
    "        \n",
    "        if interpolated.grad is not None:\n",
    "            total_gradients += interpolated.grad.detach()\n",
    "            \n",
    "    avg_gradients = total_gradients / (n_steps + 1)\n",
    "    attributions = (embeddings - baseline) * avg_gradients\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0).cpu().numpy()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "    \n",
    "    aggregated_features = {}\n",
    "    current_word = \"\"\n",
    "    current_score = 0.0\n",
    "    \n",
    "    for token, score in zip(tokens, attributions):\n",
    "        clean_tok = token.replace('Ġ', '').replace('\\u0120', '').replace('▁', '').replace('</w>', '')\n",
    "        if token in ['<s>', '</s>', '<pad>', '']:\n",
    "            continue\n",
    "            \n",
    "        if token.startswith('Ġ') or token.startswith('\\u0120') or not current_word:\n",
    "            if current_word:\n",
    "                aggregated_features[current_word] = aggregated_features.get(current_word, 0) + float(current_score)\n",
    "            current_word = clean_tok\n",
    "            current_score = float(score)\n",
    "        else:\n",
    "            current_word += clean_tok\n",
    "            current_score += float(score)\n",
    "            \n",
    "    if current_word:\n",
    "        aggregated_features[current_word] = aggregated_features.get(current_word, 0) + float(current_score)\n",
    "\n",
    "    importance = list(aggregated_features.items())\n",
    "    importance = sorted(importance, key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    return importance[:num_features], pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for idx, text in enumerate(test_samples):\n",
    "    print(f\"\\nSample {idx+1}: {text}\")\n",
    "    probs = predict_proba(text)[0]\n",
    "    pred_class = np.argmax(probs)\n",
    "    pred_label = id2label[pred_class]\n",
    "    print(f\"Prediction: {pred_label} ({probs[pred_class]:.3f})\")\n",
    "    \n",
    "    lime_features, _ = explain_with_lime(text)\n",
    "    shap_features, _ = explain_with_shap(text)\n",
    "    ig_features, _ = explain_with_ig(text)\n",
    "    \n",
    "    results.append({\n",
    "        'sample_id': idx + 1,\n",
    "        'text': text,\n",
    "        'prediction': pred_label,\n",
    "        'confidence': probs[pred_class],\n",
    "        'lime_features': lime_features[:5],\n",
    "        'shap_features': shap_features[:5],\n",
    "        'ig_features': ig_features[:5]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison(sample_result):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    method_names = ['LIME', 'SHAP', 'Integrated Gradients']\n",
    "    method_keys = ['lime_features', 'shap_features', 'ig_features']\n",
    "    \n",
    "    for idx, (method_key, method_name, ax) in enumerate(zip(method_keys, method_names, axes)):\n",
    "        features = sample_result[method_key]\n",
    "        features_sorted = sorted(features, key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        if features_sorted:\n",
    "            words, scores = zip(*features_sorted)\n",
    "            \n",
    "            sorted_pairs = sorted(zip(words, scores), key=lambda x: x[1])\n",
    "            words, scores = zip(*sorted_pairs)\n",
    "            words = list(words)\n",
    "            scores = list(scores)\n",
    "        else:\n",
    "            words, scores = [], []\n",
    "        \n",
    "        colors = ['#2ecc71' if s > 0 else '#e74c3c' for s in scores]\n",
    "        \n",
    "        ax.barh(range(len(words)), scores, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "        ax.set_yticks(range(len(words)))\n",
    "        ax.set_yticklabels(words, fontsize=12, fontweight='bold')\n",
    "        ax.set_title(method_name, fontsize=14, fontweight='bold', pad=10)\n",
    "        ax.axvline(x=0, color='black', linewidth=1.5, linestyle='--')\n",
    "        ax.grid(axis='x', alpha=0.3, linestyle=':')\n",
    "        ax.set_xlabel('Importance Score', fontsize=11)\n",
    "    \n",
    "    text_preview = sample_result['text'][:80] + '...' if len(sample_result['text']) > 80 else sample_result['text']\n",
    "    title = f\"Sample {sample_result['sample_id']}: \\\"{text_preview}\\\"\\nPrediction: {sample_result['prediction']} (Confidence: {sample_result['confidence']:.1%})\"\n",
    "    plt.suptitle(title, fontsize=12, fontweight='bold', y=1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for idx in range(len(results)):\n",
    "    visualize_comparison(results[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
