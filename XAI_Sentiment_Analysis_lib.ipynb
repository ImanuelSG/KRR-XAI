{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification Interpretability (Library-based)\n",
    "\n",
    "This notebook demonstrates interpretability techniques using standard libraries:\n",
    "1. LIME\n",
    "2. SHAP\n",
    "3. Integrated Gradients (IG) - Replaces LRP for better stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch pandas matplotlib numpy lime shap -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from transformers import RobertaTokenizer, AutoModelForSequenceClassification\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './best_roberta_model'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with open(f'{model_path}/label_mappings.json', 'r') as f:\n",
    "    label_mappings = json.load(f)\n",
    "\n",
    "label_list = label_mappings['label_list']\n",
    "label2id = label_mappings['label2id']\n",
    "id2label = {int(k): v for k, v in label_mappings['id2label'].items()}\n",
    "\n",
    "def predict_proba(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    elif isinstance(texts, np.ndarray):\n",
    "        texts = texts.tolist()\n",
    "    \n",
    "    texts = [str(t) for t in texts]\n",
    "    \n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "    return probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "    \"This movie is absolutely fantastic! I loved every moment of it.\",\n",
    "    \"Terrible experience, waste of time and money. Very disappointed.\",\n",
    "    \"The product is perfect, it exceeded all my expectations.\",\n",
    "    \"I'm extremely happy with this purchase! Highly recommend!\",\n",
    "    \"This is the worst service I've ever encountered. Absolutely horrible.\",\n",
    "    \"Bad staff. Never coming back.\",\n",
    "    \"Outstanding quality! Best investment I ever made.\",\n",
    "    \"The acting was pathetic and the plot was nonsense.\",\n",
    "    \"I adore this place, it's magical and wonderful.\",\n",
    "    \"Brilliant work! Truly exceptional and inspiring.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = LimeTextExplainer(class_names=label_list)\n",
    "\n",
    "def explain_with_lime(text, num_features=5):\n",
    "    probs = predict_proba([text])[0]\n",
    "    pred_class = int(np.argmax(probs))\n",
    "\n",
    "    exp = lime_explainer.explain_instance(\n",
    "        text,\n",
    "        predict_proba,\n",
    "        num_features=num_features,\n",
    "        num_samples=1000,\n",
    "        labels=[pred_class]\n",
    "    )\n",
    "\n",
    "    importance = exp.as_list(label=pred_class)\n",
    "    importance = sorted(importance, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "    return importance, pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.Explainer(predict_proba, shap.maskers.Text(tokenizer))\n",
    "\n",
    "def explain_with_shap(text, num_features=5):\n",
    "    shap_values = shap_explainer([text])\n",
    "    pred_class = np.argmax(predict_proba([text])[0])\n",
    "\n",
    "    values = shap_values.values[0, :, pred_class]\n",
    "    tokens = shap_values.data[0]\n",
    "\n",
    "    aggregated_features = {}\n",
    "    current_word = \"\"\n",
    "    current_score = 0.0\n",
    "    \n",
    "    for token, score in zip(tokens, values):\n",
    "        clean_tok = token.replace('Ġ', '')\n",
    "        \n",
    "        if token.startswith('Ġ') or not current_word:\n",
    "            if current_word:\n",
    "                aggregated_features[current_word] = aggregated_features.get(current_word, 0) + current_score\n",
    "            current_word = clean_tok\n",
    "            current_score = score\n",
    "        else:\n",
    "            current_word += clean_tok\n",
    "            current_score += score\n",
    "            \n",
    "    if current_word:\n",
    "        aggregated_features[current_word] = aggregated_features.get(current_word, 0) + current_score\n",
    "\n",
    "    importance = list(aggregated_features.items())\n",
    "    importance = sorted(importance, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "    return importance[:num_features], pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients (IG)\n",
    "Replacing simple LRP/Gradient-based heatmap with Integrated Gradients for better stability and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_with_ig(text, num_features=5, n_steps=50):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=64)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    embedding_layer = model.get_input_embeddings()\n",
    "    embeddings = embedding_layer(input_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "        pred_class = torch.argmax(probs).item()\n",
    "\n",
    "    baseline = torch.zeros_like(embeddings).to(device)\n",
    "    scaled_embeddings = [baseline + (float(i) / n_steps) * (embeddings - baseline) for i in range(n_steps + 1)]\n",
    "    \n",
    "    total_gradients = torch.zeros_like(embeddings)\n",
    "    \n",
    "    for i, input_embed in enumerate(scaled_embeddings):\n",
    "        input_embed.requires_grad_(True)\n",
    "        outputs = model(inputs_embeds=input_embed, attention_mask=attention_mask)\n",
    "        score = outputs.logits[0, pred_class]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        score.backward()\n",
    "        \n",
    "        if input_embed.grad is not None:\n",
    "            total_gradients += input_embed.grad\n",
    "            \n",
    "    avg_gradients = total_gradients / (n_steps + 1)\n",
    "    attributions = (embeddings - baseline) * avg_gradients\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "    \n",
    "    aggregated_features = {}\n",
    "    current_word = \"\"\n",
    "    current_score = 0.0\n",
    "    \n",
    "    for token, score in zip(tokens, attributions):\n",
    "        clean_tok = token.replace('Ġ', '').replace('</w>', '')\n",
    "        if token in ['<s>', '</s>', '<pad>', '']: \n",
    "            continue\n",
    "            \n",
    "        if token.startswith('Ġ') or not current_word:\n",
    "            if current_word:\n",
    "                aggregated_features[current_word] = aggregated_features.get(current_word, 0) + current_score\n",
    "            current_word = clean_tok\n",
    "            current_score = score\n",
    "        else:\n",
    "            current_word += clean_tok\n",
    "            current_score += score\n",
    "            \n",
    "    if current_word:\n",
    "        aggregated_features[current_word] = aggregated_features.get(current_word, 0) + current_score\n",
    "\n",
    "    importance = list(aggregated_features.items())\n",
    "    importance = sorted(importance, key=lambda x: abs(x[1]), reverse=True)\n",
    "    return importance[:num_features], pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for idx, text in enumerate(test_samples):\n",
    "    print(f\"\\nSample {idx+1}: {text}\")\n",
    "    probs = predict_proba(text)[0]\n",
    "    pred_class = np.argmax(probs)\n",
    "    pred_label = id2label[pred_class]\n",
    "    print(f\"Prediction: {pred_label} ({probs[pred_class]:.3f})\")\n",
    "    \n",
    "    lime_features, _ = explain_with_lime(text)\n",
    "    shap_features, _ = explain_with_shap(text)\n",
    "    ig_features, _ = explain_with_ig(text)\n",
    "    \n",
    "    results.append({\n",
    "        'sample_id': idx + 1,\n",
    "        'text': text,\n",
    "        'prediction': pred_label,\n",
    "        'confidence': probs[pred_class],\n",
    "        'lime_features': lime_features[:5],\n",
    "        'shap_features': shap_features[:5],\n",
    "        'ig_features': ig_features[:5]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison(sample_result):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    method_names = ['LIME', 'SHAP', 'Integrated Gradients']\n",
    "    method_keys = ['lime_features', 'shap_features', 'ig_features']\n",
    "    for idx, (method_key, method_name, ax) in enumerate(zip(method_keys, method_names, axes)):\n",
    "        features = sample_result[method_key]\n",
    "        features_sorted = sorted(features, key=lambda x: abs(x[1]), reverse=True)\n",
    "        if features_sorted:\n",
    "            words, scores = zip(*features_sorted)\n",
    "        else:\n",
    "            words, scores = [], []\n",
    "        words = list(reversed(words))\n",
    "        scores = list(reversed(scores))\n",
    "        \n",
    "        colors = ['#2ecc71' if s > 0 else '#e74c3c' for s in scores]\n",
    "        \n",
    "        ax.barh(range(len(words)), scores, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "        ax.set_yticks(range(len(words)))\n",
    "        ax.set_yticklabels(words, fontsize=12, fontweight='bold')\n",
    "        ax.set_title(method_name, fontsize=14, fontweight='bold', pad=10)\n",
    "        ax.axvline(x=0, color='black', linewidth=1.5, linestyle='--')\n",
    "        ax.grid(axis='x', alpha=0.3, linestyle=':')\n",
    "        ax.set_xlabel('Importance Score', fontsize=11)\n",
    "    text_preview = sample_result['text'][:80] + '...' if len(sample_result['text']) > 80 else sample_result['text']\n",
    "    title = f\"Sample {sample_result['sample_id']}: \\\"{text_preview}\\\"\\nPrediction: {sample_result['prediction']} (Confidence: {sample_result['confidence']:.1%})\"\n",
    "    plt.suptitle(title, fontsize=12, fontweight='bold', y=1.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "for idx in range(len(results)):\n",
    "    visualize_comparison(results[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.35, wspace=0.3)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "agreement_matrix = []\n",
    "for result in results:\n",
    "    lime_words = set([w.lower() for w, _ in result['lime_features']])\n",
    "    shap_words = set([w.lower() for w, _ in result['shap_features']])\n",
    "    ig_words = set([w.lower() for w, _ in result['ig_features']])\n",
    "    agreement_matrix.append([\n",
    "        len(lime_words & shap_words),\n",
    "        len(lime_words & ig_words),\n",
    "        len(shap_words & ig_words),\n",
    "        len(lime_words & shap_words & ig_words)\n",
    "    ])\n",
    "agreement_matrix = np.array(agreement_matrix)\n",
    "im1 = ax1.imshow(agreement_matrix.T, cmap='YlGn', aspect='auto', vmin=0, vmax=5)\n",
    "ax1.set_yticks(range(4))\n",
    "ax1.set_yticklabels(['L-S', 'L-IG', 'S-IG', 'All 3'], fontsize=11, fontweight='bold')\n",
    "ax1.set_xticks(range(len(results)))\n",
    "ax1.set_xticklabels([f\"S{r['sample_id']}\" for r in results], fontsize=10)\n",
    "ax1.set_xlabel('Sample', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Feature Agreement Between Methods', fontsize=14, fontweight='bold', pad=15)\n",
    "for i in range(4):\n",
    "    for j in range(len(results)):\n",
    "        ax1.text(j, i, int(agreement_matrix[j, i]), ha='center', va='center', color='black', fontsize=10, fontweight='bold')\n",
    "plt.colorbar(im1, ax=ax1, label='Shared Features')\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "confidences = [r['confidence'] for r in results]\n",
    "predictions = [r['prediction'] for r in results]\n",
    "colors = ['#2ecc71' if p == 'Positive' else '#e74c3c' if p == 'Negative' else '#f39c12' for p in predictions]\n",
    "ax2.bar(range(len(results)), confidences, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.set_xlabel('Sample', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Confidence', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Prediction Confidence', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(range(len(results)))\n",
    "ax2.set_xticklabels([f\"S{i+1}\" for i in range(len(results))])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "avg_agreements = agreement_matrix.mean(axis=0)\n",
    "pairs = ['L-S', 'L-IG', 'S-IG', 'All 3']\n",
    "ax3.bar(pairs, avg_agreements, color=['#3498db', '#9b59b6', '#e67e22', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
    "ax3.set_ylabel('Avg Shared Features', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Average Agreement', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylim(0, 5)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "for i, val in enumerate(avg_agreements):\n",
    "    ax3.text(i, val + 0.1, f'{val:.1f}', ha='center', fontsize=11, fontweight='bold')\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "all_features = {}\n",
    "for result in results:\n",
    "    for word, _ in (result['lime_features'][:3] + result['shap_features'][:3] + result['ig_features'][:3]):\n",
    "        word = word.lower().strip('.,!?')\n",
    "        if word:\n",
    "            all_features[word] = all_features.get(word, 0) + 1\n",
    "top_features = sorted(all_features.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "if top_features:\n",
    "    words, counts = zip(*top_features)\n",
    "    colors_freq = plt.cm.viridis(np.linspace(0.3, 0.9, len(words)))\n",
    "    ax4.barh(range(len(words)), counts, color=colors_freq, alpha=0.8, edgecolor='black')\n",
    "    ax4.set_yticks(range(len(words)))\n",
    "    ax4.set_yticklabels(words, fontsize=12, fontweight='bold')\n",
    "    ax4.set_xlabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Most Influential Features', fontsize=14, fontweight='bold')\n",
    "    ax4.invert_yaxis()\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "    for i, count in enumerate(counts):\n",
    "        ax4.text(count + 0.3, i, f'{int(count)}', va='center', fontsize=10, fontweight='bold')\n",
    "plt.suptitle('Summary Analysis', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.show()\n",
    "print(f\"Average confidence: {np.mean(confidences):.1%}\")\n",
    "print(f\"Average agreement (all 3): {avg_agreements[3]:.2f} features\")\n",
    "if top_features:\n",
    "    print(f\"Most influential word: '{top_features[0][0]}' ({top_features[0][1]} appearances)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
