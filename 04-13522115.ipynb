{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d801d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer, RobertaTokenizer, XLMRobertaTokenizer,\n",
    "    AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c4a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e5ad9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_df, test_size=0.2, stratify=train_df['Sentiment'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0afa02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "\n",
    "label_list = sorted(train_df['Sentiment'].unique())\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "def encode_labels(example):\n",
    "    example['labels'] = label2id[example['Sentiment']]\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(encode_labels)\n",
    "val_dataset = val_dataset.map(encode_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df93ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tok = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "roberta_tok = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "xlmr_tok = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c12cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_bert(example):\n",
    "    return bert_tok(example['Text'], truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "def tok_roberta(example):\n",
    "    return roberta_tok(example['Text'], truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "def tok_xlmr(example):\n",
    "    return xlmr_tok(example['Text'], truncation=True, padding='max_length', max_length=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6596d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_tok.tokenize(train_dataset[0]['Text']))\n",
    "print(roberta_tok.tokenize(train_dataset[0]['Text']))\n",
    "print(xlmr_tok.tokenize(train_dataset[0]['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4fb996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = preds.argmax(axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n",
    "\n",
    "def train_and_eval(model_name, tokenizer_func):\n",
    "    tokenized_train = train_dataset.map(tokenizer_func, batched=True).remove_columns(['Text'])\n",
    "    tokenized_val = val_dataset.map(tokenizer_func, batched=True).remove_columns(['Text'])\n",
    "    tokenized_train.set_format('torch')\n",
    "    tokenized_val.set_format('torch')\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./results/{model_name}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        disable_tqdm=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=bert_tok,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return trainer.evaluate(), trainer.state.log_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "loss = {}\n",
    "\n",
    "results['BERT'], loss['BERT'] = train_and_eval('bert-base-cased', tok_bert)\n",
    "results['RoBERTa'], loss['RoBERTa'] = train_and_eval('roberta-base', tok_roberta)\n",
    "results['XLM-R'], loss['XLM-R'] = train_and_eval('xlm-roberta-base', tok_xlmr)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b94ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(loss_dict, cols=3):\n",
    "    n_models = len(loss_dict)\n",
    "    rows = math.ceil(n_models / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(6 * cols, 4 * rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (model_name, log_history) in enumerate(loss_dict.items()):\n",
    "        log_history = pd.DataFrame(log_history)\n",
    "        train_loss = log_history[log_history['loss'].notna()]\n",
    "        eval_loss = log_history[log_history['eval_loss'].notna()]\n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.plot(train_loss['step'], train_loss['loss'], label='Train Loss', color='tab:blue')\n",
    "        ax.plot(eval_loss['step'], eval_loss['eval_loss'], label='Val Loss', color='tab:orange', linestyle='--')\n",
    "        ax.set_title(f\"{model_name}\")\n",
    "        ax.set_xlabel(\"Training Step\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.grid(True, linestyle=\":\")\n",
    "        ax.legend()\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18f366d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
