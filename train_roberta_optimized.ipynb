{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# RoBERTa Training for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch pandas scikit-learn evaluate accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "train, val = train_test_split(train_df, test_size=0.2, stratify=train_df['Sentiment'], random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train)}, Val: {len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = sorted(train_df['Sentiment'].unique())\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "def encode_labels(example):\n",
    "    example['labels'] = label2id[example['Sentiment']]\n",
    "    return example\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train).map(encode_labels)\n",
    "val_dataset = Dataset.from_pandas(val).map(encode_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['Text'], truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True).remove_columns(['Text'])\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True).remove_columns(['Text'])\n",
    "\n",
    "tokenized_train.set_format('torch')\n",
    "tokenized_val.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"precision\": precision.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'roberta-base', \n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/roberta-base\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "train_result = trainer.train()\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nTraining time: {int(elapsed//60)}m {int(elapsed%60)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Accuracy:  {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1:        {eval_results['eval_f1']:.4f}\")\n",
    "print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall:    {eval_results['eval_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"./best_roberta_model\"\n",
    "\n",
    "trainer.save_model(best_model_path)\n",
    "tokenizer.save_pretrained(best_model_path)\n",
    "\n",
    "label_mappings = {\n",
    "    'label2id': label2id,\n",
    "    'id2label': id2label,\n",
    "    'label_list': label_list\n",
    "}\n",
    "\n",
    "with open(f\"{best_model_path}/label_mappings.json\", 'w') as f:\n",
    "    json.dump(label_mappings, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
    "loaded_tokenizer = RobertaTokenizer.from_pretrained(best_model_path)\n",
    "\n",
    "with open(f\"{best_model_path}/label_mappings.json\", 'r') as f:\n",
    "    loaded_labels = json.load(f)\n",
    "\n",
    "test_texts = [\n",
    "    \"This is absolutely amazing! I love it!\",\n",
    "    \"Terrible experience, very disappointed.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "\n",
    "loaded_model.eval()\n",
    "\n",
    "for text in test_texts:\n",
    "    inputs = loaded_tokenizer(text, return_tensors='pt', truncation=True, max_length=64)\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "        pred_class = outputs.logits.argmax(dim=-1).item()\n",
    "    \n",
    "    pred_label = loaded_labels['id2label'][str(pred_class)]\n",
    "    print(f\"{text[:50]:50s} -> {pred_label} ({probs[0, pred_class]:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
