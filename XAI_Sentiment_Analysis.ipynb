{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable AI for Sentiment Classification\n",
    "\n",
    "This notebook implements three XAI methods:\n",
    "1. LIME (Local Interpretable Model-agnostic Explanations)\n",
    "2. SHAP (SHapley Additive exPlanations)\n",
    "3. Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Imanuel Girsang\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch pandas matplotlib numpy scikit-learn scipy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from transformers import RobertaTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.special import comb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './best_roberta_model'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with open(f'{model_path}/label_mappings.json', 'r') as f:\n",
    "    label_mappings = json.load(f)\n",
    "\n",
    "label_list = label_mappings['label_list']\n",
    "label2id = label_mappings['label2id']\n",
    "id2label = {int(k): v for k, v in label_mappings['id2label'].items()}\n",
    "\n",
    "def predict_sentiment(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "    return probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "    \"This movie is absolutely fantastic! I loved every moment of it.\",\n",
    "    \"Terrible experience, waste of time and money. Very disappointed.\",\n",
    "    \"The product is okay, nothing special but does the job.\",\n",
    "    \"I'm extremely happy with this purchase! Highly recommend!\",\n",
    "    \"This is the worst service I've ever encountered. Absolutely horrible.\",\n",
    "    \"Not bad, could be better but acceptable for the price.\",\n",
    "    \"Outstanding quality! Exceeded all my expectations.\",\n",
    "    \"Mediocre at best, wouldn't buy again.\",\n",
    "    \"I feel neutral about this, it's just average.\",\n",
    "    \"Brilliant work! Truly exceptional and inspiring.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIMEExplainer:\n",
    "    def __init__(self, predict_fn, num_samples=1000):\n",
    "        self.predict_fn = predict_fn\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "    \n",
    "    def kernel_fn(self, distances):\n",
    "        return np.sqrt(np.exp(-(distances ** 2) / 25 ** 2))\n",
    "    \n",
    "    def explain(self, text, target_class=None):\n",
    "        words = self.tokenize(text)\n",
    "        n_words = len(words)\n",
    "        \n",
    "        perturbations = np.random.binomial(1, 0.5, (self.num_samples, n_words))\n",
    "        perturbed_texts = [' '.join([w for w, m in zip(words, mask) if m == 1]) or '' \n",
    "                          for mask in perturbations]\n",
    "        \n",
    "        predictions = self.predict_fn(perturbed_texts)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = np.argmax(self.predict_fn([text])[0])\n",
    "        \n",
    "        y = predictions[:, target_class]\n",
    "        distances = np.sum(1 - perturbations, axis=1)\n",
    "        weights = self.kernel_fn(distances)\n",
    "        \n",
    "        ridge = Ridge(alpha=1.0)\n",
    "        ridge.fit(perturbations, y, sample_weight=weights)\n",
    "        \n",
    "        importance = list(zip(words, ridge.coef_))\n",
    "        importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        return importance, target_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHAPExplainer:\n",
    "    def __init__(self, predict_fn, num_samples=500):\n",
    "        self.predict_fn = predict_fn\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def shapley_kernel(self, n, s):\n",
    "        if s == 0 or s == n:\n",
    "            return 1e10\n",
    "        return (n - 1) / (comb(n, s) * s * (n - s))\n",
    "    \n",
    "    def explain(self, text, target_class=None):\n",
    "        words = text.split()\n",
    "        n_words = len(words)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = np.argmax(self.predict_fn([text])[0])\n",
    "        \n",
    "        empty_pred = self.predict_fn([''])[0, target_class]\n",
    "        shap_values = np.zeros(n_words)\n",
    "        \n",
    "        for _ in range(self.num_samples):\n",
    "            z = np.random.binomial(1, 0.5, n_words)\n",
    "            for i in range(n_words):\n",
    "                z_with, z_without = z.copy(), z.copy()\n",
    "                z_with[i], z_without[i] = 1, 0\n",
    "                \n",
    "                text_with = ' '.join([w for w, m in zip(words, z_with) if m == 1])\n",
    "                text_without = ' '.join([w for w, m in zip(words, z_without) if m == 1])\n",
    "                \n",
    "                pred_with = self.predict_fn([text_with])[0, target_class] if text_with else empty_pred\n",
    "                pred_without = self.predict_fn([text_without])[0, target_class] if text_without else empty_pred\n",
    "                \n",
    "                contrib = pred_with - pred_without\n",
    "                weight = self.shapley_kernel(n_words, np.sum(z_without))\n",
    "                shap_values[i] += contrib * weight\n",
    "        \n",
    "        shap_values /= self.num_samples\n",
    "        importance = list(zip(words, shap_values))\n",
    "        importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        return importance, target_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedGradientsExplainer:\n",
    "    def __init__(self, model, tokenizer, device, n_steps=50):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.n_steps = n_steps\n",
    "    \n",
    "    def explain(self, text, target_class=None):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=64)\n",
    "        input_ids = inputs['input_ids'].to(self.device)\n",
    "        attention_mask = inputs['attention_mask'].to(self.device)\n",
    "        \n",
    "        if target_class is None:\n",
    "            with torch.no_grad():\n",
    "                target_class = self.model(input_ids=input_ids, attention_mask=attention_mask).logits.argmax().item()\n",
    "        \n",
    "        embeddings = self.model.roberta.embeddings(input_ids)\n",
    "        embeddings.requires_grad_(True)\n",
    "        baseline = torch.zeros_like(embeddings)\n",
    "        \n",
    "        gradients = []\n",
    "        for i in range(self.n_steps + 1):\n",
    "            scaled_input = baseline + (float(i) / self.n_steps) * (embeddings - baseline)\n",
    "            scaled_input.requires_grad_(True)\n",
    "            \n",
    "            outputs = self.model(inputs_embeds=scaled_input, attention_mask=attention_mask)\n",
    "            score = outputs.logits[0, target_class]\n",
    "            self.model.zero_grad()\n",
    "            score.backward(retain_graph=True)\n",
    "            gradients.append(scaled_input.grad.clone())\n",
    "        \n",
    "        avg_gradients = torch.stack(gradients).mean(dim=0)\n",
    "        integrated_gradients = (embeddings - baseline) * avg_gradients\n",
    "        attributions = integrated_gradients.sum(dim=-1).squeeze(0).cpu().detach().numpy()\n",
    "        \n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "        importance = [(t, a) for t, a in zip(tokens, attributions) if t not in ['<s>', '</s>', '<pad>']]\n",
    "        importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        return importance, target_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = LIMEExplainer(predict_sentiment, num_samples=1000)\n",
    "shap_explainer = SHAPExplainer(predict_sentiment, num_samples=500)\n",
    "ig_explainer = IntegratedGradientsExplainer(model, tokenizer, device, n_steps=50)\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, text in enumerate(test_samples):\n",
    "    print(f\"\\nSample {idx+1}: {text}\")\n",
    "    \n",
    "    probs = predict_sentiment(text)[0]\n",
    "    pred_class = np.argmax(probs)\n",
    "    pred_label = id2label[pred_class]\n",
    "    \n",
    "    print(f\"Prediction: {pred_label} ({probs[pred_class]:.3f})\")\n",
    "    \n",
    "    lime_features, _ = lime_explainer.explain(text)\n",
    "    shap_features, _ = shap_explainer.explain(text)\n",
    "    ig_features, _ = ig_explainer.explain(text)\n",
    "    \n",
    "    results.append({\n",
    "        'sample_id': idx + 1,\n",
    "        'text': text,\n",
    "        'prediction': pred_label,\n",
    "        'confidence': probs[pred_class],\n",
    "        'lime_features': lime_features[:5],\n",
    "        'shap_features': shap_features[:5],\n",
    "        'ig_features': ig_features[:5]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison(sample_result):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for idx, (method, ax) in enumerate(zip(['lime_features', 'shap_features', 'ig_features'], axes)):\n",
    "        words, scores = zip(*sample_result[method])\n",
    "        colors = ['green' if s > 0 else 'red' for s in scores]\n",
    "        ax.barh(range(len(words)), scores, color=colors, alpha=0.6)\n",
    "        ax.set_yticks(range(len(words)))\n",
    "        ax.set_yticklabels(words)\n",
    "        ax.set_title(['LIME', 'SHAP', 'Integrated Gradients'][idx])\n",
    "        ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    text_preview = sample_result['text'][:60] + '...' if len(sample_result['text']) > 60 else sample_result['text']\n",
    "    plt.suptitle(f'{text_preview}\\nPrediction: {sample_result[\"prediction\"]} ({sample_result[\"confidence\"]:.3f})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for idx in [0, 1, 4, 9]:\n",
    "    visualize_comparison(results[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Agreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_stats = []\n",
    "\n",
    "for result in results:\n",
    "    lime_words = set([w.lower() for w, _ in result['lime_features']])\n",
    "    shap_words = set([w.lower() for w, _ in result['shap_features']])\n",
    "    ig_words = set([w.lower() for w, _ in result['ig_features']])\n",
    "    \n",
    "    all_three = lime_words & shap_words & ig_words\n",
    "    \n",
    "    agreement_stats.append({\n",
    "        'sample_id': result['sample_id'],\n",
    "        'all_three': len(all_three),\n",
    "        'lime_shap': len(lime_words & shap_words),\n",
    "        'lime_ig': len(lime_words & ig_words),\n",
    "        'shap_ig': len(shap_words & ig_words),\n",
    "        'agreed_features': list(all_three)\n",
    "    })\n",
    "\n",
    "for stat in agreement_stats:\n",
    "    print(f\"\\nSample {stat['sample_id']}: {stat['all_three']} features agreed by all three methods\")\n",
    "    print(f\"  Features: {stat['agreed_features']}\")\n",
    "\n",
    "avg = np.mean([s['all_three'] for s in agreement_stats])\n",
    "print(f\"\\nAverage agreement: {avg:.2f} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame([{\n",
    "    'Sample': r['sample_id'],\n",
    "    'Text': r['text'][:40] + '...',\n",
    "    'Prediction': r['prediction'],\n",
    "    'Confidence': f\"{r['confidence']:.3f}\",\n",
    "    'LIME Top': r['lime_features'][0][0],\n",
    "    'SHAP Top': r['shap_features'][0][0],\n",
    "    'IG Top': r['ig_features'][0][0]\n",
    "} for r in results])\n",
    "\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-xai-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
